{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\n!pip install --upgrade wandb\n!pip install torchmetrics\n'''\n!pip install GPUtil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-17T10:53:00.308922Z","iopub.execute_input":"2022-07-17T10:53:00.309348Z","iopub.status.idle":"2022-07-17T10:53:09.486692Z","shell.execute_reply.started":"2022-07-17T10:53:00.309260Z","shell.execute_reply":"2022-07-17T10:53:09.485535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport gc\nimport os\nfrom tqdm import tqdm\nfrom datasets import  Dataset as DS, DatasetDict,load_from_disk\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom torch.utils.data import Dataset, DataLoader \nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AdamW, DataCollatorWithPadding\nfrom GPUtil import showUtilization as gpu_usage\nfrom torchmetrics import F1Score\nfrom IPython.display import FileLink\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-07-17T10:53:09.488767Z","iopub.execute_input":"2022-07-17T10:53:09.489087Z","iopub.status.idle":"2022-07-17T10:53:12.247475Z","shell.execute_reply.started":"2022-07-17T10:53:09.489059Z","shell.execute_reply":"2022-07-17T10:53:12.246487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_config():\n    CONFIG = {\n    'train_directory': \"../input/feedback-prize-effectiveness/train\",\n    'test_directory': \"../input/feedback-prize-effectiveness/test\",\n    'path_to_processed_datasets' : 'dataset_dict',\n    'seed': 42,\n    'device': torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n    'model_name': 'microsoft/deberta-v3-base',\n    'epoches': 1,\n    'folds': 5,\n    'lr': 1e-5,\n    'batch_size' : 12,\n    'num_classes': 3,\n    'max_length':512,\n    'max_iters_for_scheduler': 500,\n    'min_lr': 1e-6,\n    'optimizer_weight_decay':1e-6\n    }\n    CONFIG['tokenizer'] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n    CONFIG['metric'] = F1Score(num_classes=CONFIG['num_classes'])\n    return CONFIG\n\nCONFIG = init_config()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T10:53:12.249018Z","iopub.execute_input":"2022-07-17T10:53:12.249912Z","iopub.status.idle":"2022-07-17T10:53:20.076530Z","shell.execute_reply.started":"2022-07-17T10:53:12.249855Z","shell.execute_reply":"2022-07-17T10:53:20.075540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    np.random.seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-07-17T10:53:20.079700Z","iopub.execute_input":"2022-07-17T10:53:20.080210Z","iopub.status.idle":"2022-07-17T10:53:20.088574Z","shell.execute_reply.started":"2022-07-17T10:53:20.080170Z","shell.execute_reply":"2022-07-17T10:53:20.087417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetForPredictionQualityOfArgumentation(Dataset):    \n    def __init__(self,path_to_train_data, path_to_test_data,  CONFIG, is_train=True):\n        self.tokenizer = CONFIG['tokenizer']\n        self.max_length = CONFIG['max_length']\n        self.path_to_train_directory = path_to_train_data\n        self.path_to_test_directory = path_to_test_data\n        \n    def tokenize_text(self, row):\n        return self.tokenizer(row[\"text\"], padding='max_length', truncation=True, add_special_tokens=True, max_length=self.max_length,return_token_type_ids=False)\n    \n    def get_essay(self, essay_id):\n        essay_path = os.path.join(self.path_to_data, f'{essay_id}.txt')\n        essay_text = open(essay_path, 'r').read()\n        return essay_text\n    \n    def process_dataset(self, df, is_train_dataset=True):\n        if is_train_dataset:\n            self.label_encoder = LabelEncoder()\n            df['labels'] = self.label_encoder.fit_transform(df['discourse_effectiveness'])\n            gfk = GroupKFold(n_splits = CONFIG['folds'])        \n            for fold, (_, val) in enumerate(gfk.split(X=df, groups = df['essay_id'])):\n                df.loc[val, 'n_fold'] = int(fold)\n            df[\"n_fold\"] = df[\"n_fold\"].astype(int)\n            df['n_fold'].value_counts()\n        \n        df['essay_text'] = df['essay_id'].apply(self.get_essay)\n        df['text'] = df['discourse_type'] + self.tokenizer.sep_token + df['discourse_text']#  + self.tokenizer.sep_token + df['essay_text']\n        \n        \n        df = DS.from_pandas(df)\n        \n        to_remove = ['discourse_text','discourse_type', 'discourse_id', 'text','essay_id', 'essay_text']\n        \n        if is_train_dataset:\n            to_remove.append('discourse_effectiveness')\n            \n        df = df.map(self.tokenize_text, batched=True, remove_columns=to_remove)\n        \n        return df\n        \n    def process_train_test_datasets(self):\n        self.path_to_processed_datasets = CONFIG['path_to_processed_datasets']\n        \n        self.path_to_data = self.path_to_train_directory\n        self.train = pd.read_csv(''.join([self.path_to_data, \".csv\"]))\n        self.train = self.process_dataset(self.train)\n        \n        self.path_to_data = self.path_to_test_directory\n        self.test = pd.read_csv(''.join([self.path_to_data, \".csv\"]))\n        self.test = self.process_dataset(self.test, is_train_dataset=False)\n        \n        dataset_dict = DatasetDict({'train': self.train, 'test': self.test})\n        dataset_dict.save_to_disk(self.path_to_processed_datasets)\n        \n        return self.path_to_processed_datasets\n        \n    def load_datasets_from_file(self, path=None):\n        if path is None:\n            self.path_to_processed_datasets = CONFIG['path_to_processed_datasets']\n        else:\n            self.path_to_processed_datasets = path\n        datasets = load_from_disk(self.path_to_processed_datasets)\n        return datasets['train'], datasets['test']\n    \n    def __getitem__(self, ind):\n        return self.df[ind]\n    \n    def __len__(self):\n        return self.df.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-17T10:53:20.090168Z","iopub.execute_input":"2022-07-17T10:53:20.090573Z","iopub.status.idle":"2022-07-17T10:53:20.113197Z","shell.execute_reply.started":"2022-07-17T10:53:20.090535Z","shell.execute_reply":"2022-07-17T10:53:20.112333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = DatasetForPredictionQualityOfArgumentation(CONFIG['train_directory'],CONFIG['test_directory'], CONFIG)\nds.process_train_test_datasets()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T10:53:20.116231Z","iopub.execute_input":"2022-07-17T10:53:20.116513Z","iopub.status.idle":"2022-07-17T10:54:05.749686Z","shell.execute_reply.started":"2022-07-17T10:53:20.116477Z","shell.execute_reply":"2022-07-17T10:54:05.748783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = ds.load_datasets_from_file()","metadata":{"execution":{"iopub.status.busy":"2022-07-17T10:54:05.751149Z","iopub.execute_input":"2022-07-17T10:54:05.752087Z","iopub.status.idle":"2022-07-17T10:54:05.762156Z","shell.execute_reply.started":"2022-07-17T10:54:05.752044Z","shell.execute_reply":"2022-07-17T10:54:05.761198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelForPredictionQualityOfArgumentation():\n    def __init__(self,ds, CONFIG):\n        self.ds = ds\n        self.lr = CONFIG['lr']\n        self.tokenizer = CONFIG['tokenizer']\n        self.collator = DataCollatorWithPadding(tokenizer=self.tokenizer, max_length=CONFIG['max_length'])\n        self.metric = CONFIG['metric'].to(CONFIG['device'])\n        self.criterion = nn.CrossEntropyLoss()\n        self.model = AutoModelForSequenceClassification.from_pretrained(CONFIG['model_name'], num_labels=CONFIG['num_classes']).to(CONFIG['device'])\n        self.optimizer = AdamW(params=self.model.parameters(),lr=self.lr, weight_decay=CONFIG['optimizer_weight_decay'])\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=CONFIG['max_iters_for_scheduler'],eta_min=CONFIG['min_lr'])\n        self.softmax = torch.nn.Softmax()\n        \n    def create_dataloader_on_full_dataset(self, dataset):\n        dataset = dataset.to_pandas()\n        if 'n_fold' in dataset.columns:\n            dataset.drop(labels=['n_fold'], axis=1, inplace=True)\n        dataset = DS.from_pandas(dataset, preserve_index=False)\n        \n        dataloader = DataLoader(\n            dataset, shuffle=True, batch_size=CONFIG['batch_size'], drop_last=True, collate_fn=self.collator, pin_memory=True\n        )\n        return dataloader\n        \n    def create_data_loaders_for_cross_val(self, num_fold):\n        ds_ = self.ds.to_pandas()\n        ds_train = ds_[ds_['n_fold'] != num_fold]\n        ds_train.drop(labels=['n_fold'], axis=1, inplace=True)\n        ds_test = ds_[ds_['n_fold'] == num_fold]\n        ds_test.drop(labels=['n_fold'], axis=1, inplace=True)\n        \n        ds_train = DS.from_pandas(ds_train, preserve_index=False)\n        ds_test =  DS.from_pandas(ds_test, preserve_index=False)\n        \n        train_dataloader = DataLoader(\n            ds_train, shuffle=True, batch_size=CONFIG['batch_size'], drop_last=True, collate_fn=self.collator, pin_memory=True\n        )\n        test_dataloader = DataLoader(\n            ds_test, shuffle=True, batch_size=CONFIG['batch_size'], drop_last=True, collate_fn=self.collator, pin_memory=True\n        )\n        \n        return train_dataloader, test_dataloader\n    \n    def train_one_epoch(self, dataloader):\n        self.model.train()\n        ds_size = 0.\n        sum_loss = 0.\n        mean_metric = 0.\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for _ , data in bar:\n            for k, v in data.items():\n                data[k] = v.to(CONFIG['device'])\n            \n            pred = self.model(**data)\n            pred = self.softmax(pred['logits'])\n            loss = self.criterion(pred, data['labels'])\n            loss.backward()\n            metric_score = self.metric(pred, data['labels']).item()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n            self.scheduler.step()\n            loss_ = loss.item()\n            for k, v in data.items():\n                data[k] = v.detach().cpu()\n            pred = pred.detach().cpu()\n            loss = loss.detach().cpu()\n            del data, pred, v, k, loss\n            gc.collect()\n            ds_size += CONFIG['batch_size']\n            sum_loss += (loss_ * CONFIG['batch_size'])\n            mean_metric += (metric_score * CONFIG['batch_size'])\n            torch.cuda.empty_cache()\n            bar.set_postfix(mean_train_loss=sum_loss/ds_size, current_train_loss=loss_,  mean_metric=mean_metric/ds_size, current_metric=metric_score) \n        return sum_loss / ds_size\n    \n    @torch.no_grad()\n    def valid_one_epoch(self, dataloader):\n        self.model.eval()\n        ds_size = 0.\n        sum_loss = 0.\n        mean_metric = 0.\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for _ , data in bar:\n            for k, v in data.items():\n                data[k] = v.to(CONFIG['device'])\n            self.optimizer.zero_grad()\n            pred = self.model(**data)\n            pred = self.softmax(pred['logits'])\n            metric_score = self.metric(pred, data['labels']).item()\n            loss = self.criterion(pred, data['labels'])\n            loss_ = loss.item()\n            for k, v in data.items():\n                data[k] = v.detach().cpu()\n            pred = v.detach().cpu()\n            loss = loss.detach().cpu()\n            del data, pred, v, k, loss\n            gc.collect()\n            ds_size += CONFIG['batch_size']\n            sum_loss += (loss_ * CONFIG['batch_size'])\n            mean_metric += (metric_score * CONFIG['batch_size'])\n            torch.cuda.empty_cache()\n            bar.set_postfix(mean_valid_loss=sum_loss/ds_size, current_valid_loss=loss_,  mean_metric=mean_metric/ds_size, current_metric=metric_score) \n        return sum_loss / ds_size\n    \n    def run_learning(self, cross_val=True):\n        min_loss_val = np.inf \n        mean_loss_train = 0.\n        mean_loss_test = 0.\n        if cross_val:\n            for i in range(CONFIG['folds']):\n                print('fold: ', i)\n                train_loader, test_loader = self.create_data_loaders_for_cross_val(i)\n                for i in range(CONFIG['epoches']):\n                    loss_train = self.train_one_epoch(train_loader)\n                    loss_val = self.valid_one_epoch(test_loader)\n                    if min_loss_val <= loss_val:\n                        path = f'model-fold: {fold}.bin'\n                        torch.save(self.model.state_dict(), path)\n                mean_loss_train += loss_train\n                mean_loss_test += loss_val\n                        \n                self.model = AutoModelForSequenceClassification.from_pretrained(CONFIG['model_name'], num_labels=CONFIG['num_classes']).to(CONFIG['device'])\n                self.optimizer = AdamW(params=self.model.parameters(),lr=self.lr, weight_decay=CONFIG['optimizer_weight_decay'])\n                self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=CONFIG['max_iters_for_scheduler'], eta_min=CONFIG['min_lr'])\n                torch.cuda.empty_cache()\n            print(f\"loss train/test: {mean_loss_train} {mean_loss_test}\")\n        else:\n            train_loader = self.create_dataloader_on_full_dataset(self.ds)\n            for i in range(CONFIG['epoches']):\n                loss_train = self.train_one_epoch(train_loader)\n                self.path_to_weights = 'weights_for_bert.bin'\n                torch.save(self.model.state_dict(), path)\n                \n    def download_weights_for_model(self):\n        self.model.load_state_dict(torch.load(self.path_to_weights))\n        \n    def create_download_link(self, df, title = \"Download CSV file\", filename = \"DBV3L.csv\"):  \n        csv = df.to_csv(filename, index=False)\n        return FileLink(filename)\n    \n    @torch.no_grad()\n    def final_prediction_for_kaggle(self, test_dataset, is_download_weights=False):\n        batch_size = 10\n        test_loader = DataLoader(\n            test_dataset, batch_size=batch_size, collate_fn=self.collator, pin_memory=True\n        )\n        \n        if is_download_weights:\n            self.download_weights_for_model()\n\n        self.model.eval()\n        bar = tqdm(enumerate(test_loader), total=len(test_loader))\n        for _ , data in bar:\n            for k, v in data.items():\n                data[k] = v.to(CONFIG['device'])\n            pred = self.model(**data)\n            pred = self.softmax(pred['logits'])\n        result = np.array(pred.tolist())\n        res = pd.DataFrame(result, columns=['Adequate', 'Effective', 'Ineffective'])\n        ids =pd.read_csv('../input/feedback-prize-effectiveness/test.csv')['discourse_id'] \n        res['discourse_id'] = ids\n        res = res[['discourse_id', 'Ineffective', 'Adequate', 'Effective']]\n        return self.create_download_link(res) ","metadata":{"execution":{"iopub.status.busy":"2022-07-17T10:54:05.763812Z","iopub.execute_input":"2022-07-17T10:54:05.764364Z","iopub.status.idle":"2022-07-17T10:54:05.803733Z","shell.execute_reply.started":"2022-07-17T10:54:05.764327Z","shell.execute_reply":"2022-07-17T10:54:05.802805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ModelForPredictionQualityOfArgumentation(train, CONFIG)\nmodel.run_learning(cross_val=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-17T10:54:05.805224Z","iopub.execute_input":"2022-07-17T10:54:05.805620Z","iopub.status.idle":"2022-07-17T11:54:39.649414Z","shell.execute_reply.started":"2022-07-17T10:54:05.805583Z","shell.execute_reply":"2022-07-17T11:54:39.646358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = model.final_prediction_for_kaggle(test)\nres","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def release_cpu_and_gpu_memory(model):\n    print('before')\n    gpu_usage()\n    \n    model.model.to('cpu')\n    del model\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    print('after')\n    gpu_usage()\nrelease_cpu_and_gpu_memory(model)\nCONFIG = init_config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}